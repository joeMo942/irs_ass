{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Item-Based Collaborative Filtering\n\n---\n\n## Case Study 1: Item-Based CF with Cosine Similarity and DS Filtering\n\n### Overview\n\nThis notebook implements item-based collaborative filtering with:\n1. Cosine similarity with mean-centering (manual calculation)\n2. Top 20% similar items selection\n3. Rating prediction using similarity weights\n4. DF/DS metric computation (DF=53, DS=min(corated_users, DF)/DF)\n5. DS-based neighborhood filtering\n6. Comparative analysis\n\n**Note**: Similarity is computed ONLY for specified target items to minimize memory usage.\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Target Items\n\n**Modify this list to specify which items you want to analyze:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TARGET ITEMS - Specify the items you want to analyze\n# These are the items for which we will compute similarity with all other items\n\nTARGET_ITEMS = [\n    'B00YSG2ZPA',  # Most popular item in dataset\n    'B001E5DW2E',\n    'B0000AQRST',\n    'B00005JKZY',\n    'B00005JL2M',\n    'B00004RFKU',\n    'B00003CX5P',\n    'B00005JNBQ',\n    'B00004Y1H1',\n    'B00008OE6I'\n]\n\nprint(f\"Number of target items: {len(TARGET_ITEMS)}\")\nprint(f\"\\nTarget items:\")\nfor item in TARGET_ITEMS:\n    print(f\"  - {item}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set visualization style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\nnp.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 1. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\nfile_path = \"../../dataset/Movies_and_TV.csv\"\ndf = pd.read_csv(file_path)\n\n# Set proper column headers and drop timestamp\ndf.columns = ['item', 'user', 'rating', 'timestamp']\ndf = df.drop('timestamp', axis=1)\n\nprint(f\"Dataset loaded successfully!\")\nprint(f\"Shape: {df.shape}\")\nprint(f\"\\nFirst 5 rows:\")\ndf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset statistics\nprint(\"=\"*60)\nprint(\"DATASET STATISTICS\")\nprint(\"=\"*60)\nprint(f\"Total ratings: {len(df):,}\")\nprint(f\"Unique users: {df['user'].nunique():,}\")\nprint(f\"Unique items: {df['item'].nunique():,}\")\nprint(f\"Rating range: {df['rating'].min()} - {df['rating'].max()}\")\nprint(f\"Average rating: {df['rating'].mean():.2f}\")\nprint(f\"Sparsity: {(1 - len(df) / (df['user'].nunique() * df['item'].nunique())) * 100:.4f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify target items exist in dataset\nprint(\"Verifying target items exist in dataset...\")\nall_items_in_dataset = set(df['item'].unique())\nmissing_items = [item for item in TARGET_ITEMS if item not in all_items_in_dataset]\n\nif missing_items:\n    print(f\"\\nWARNING: {len(missing_items)} target items not found in dataset:\")\n    for item in missing_items:\n        print(f\"  - {item}\")\n    TARGET_ITEMS = [item for item in TARGET_ITEMS if item in all_items_in_dataset]\n    print(f\"\\nProceeding with {len(TARGET_ITEMS)} valid target items\")\nelse:\n    print(f\"✓ All {len(TARGET_ITEMS)} target items found in dataset\")\n\n# Show statistics for target items\nprint(f\"\\nTarget item statistics:\")\ntarget_item_stats = df[df['item'].isin(TARGET_ITEMS)].groupby('item')['rating'].agg(['count', 'mean'])\nprint(target_item_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all unique items\nall_items = df['item'].unique()\nnum_items = len(all_items)\n\nprint(f\"Total items in dataset: {num_items:,}\")\nprint(f\"Target items to analyze: {len(TARGET_ITEMS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step 1: Item-Based CF with Cosine Similarity (Manual Calculation with Mean-Centering)\n\nWe will calculate similarity manually using:\n\n**Cosine Similarity Formula:**\n```\nsimilarity(i, j) = dot(centered_i, centered_j) / (||centered_i|| * ||centered_j||)\n```\n\nWhere `centered_i` is the mean-centered rating vector for item i."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate item mean ratings\nprint(\"Calculating item mean ratings...\")\nitem_means = df.groupby('item')['rating'].mean().to_dict()\n\nprint(f\"Item means calculated for {len(item_means)} items\")\nprint(f\"Average item rating: {np.mean(list(item_means.values())):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build item-user rating dictionaries for fast lookup\nprint(\"Building item-user rating dictionaries...\")\n\n# For each item, store {user: rating}\nitem_user_ratings = defaultdict(dict)\nfor _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building lookups\"):\n    item_user_ratings[row['item']][row['user']] = row['rating']\n\nprint(f\"\\n✓ Lookup dictionaries created for {len(item_user_ratings)} items\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_cosine_similarity_manual(item_i, item_j, item_user_ratings, item_means):\n    \"\"\"\n    Manually compute cosine similarity between two items using mean-centered ratings.\n    \n    Args:\n        item_i: First item ID\n        item_j: Second item ID\n        item_user_ratings: Dict of {item: {user: rating}}\n        item_means: Dict of {item: mean_rating}\n    \n    Returns:\n        Cosine similarity score (0 if no common users)\n    \"\"\"\n    # Get users who rated both items\n    users_i = set(item_user_ratings[item_i].keys())\n    users_j = set(item_user_ratings[item_j].keys())\n    common_users = users_i & users_j\n    \n    if len(common_users) == 0:\n        return 0.0\n    \n    # Get mean-centered ratings for common users\n    centered_i = []\n    centered_j = []\n    \n    for user in common_users:\n        rating_i = item_user_ratings[item_i][user]\n        rating_j = item_user_ratings[item_j][user]\n        \n        centered_i.append(rating_i - item_means[item_i])\n        centered_j.append(rating_j - item_means[item_j])\n    \n    centered_i = np.array(centered_i)\n    centered_j = np.array(centered_j)\n    \n    # Compute cosine similarity: dot product / (norm_i * norm_j)\n    dot_product = np.dot(centered_i, centered_j)\n    norm_i = np.linalg.norm(centered_i)\n    norm_j = np.linalg.norm(centered_j)\n    \n    if norm_i == 0 or norm_j == 0:\n        return 0.0\n    \n    similarity = dot_product / (norm_i * norm_j)\n    \n    return similarity\n\nprint(\"✓ Manual cosine similarity function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate similarity ONLY for target items vs all items\nprint(f\"Calculating similarity for {len(TARGET_ITEMS)} target items vs all {num_items:,} items...\")\nprint(\"This will take several minutes...\\n\")\n\n# Store similarities: {target_item: {other_item: similarity}}\ntarget_similarities = {}\n\nfor target_item in tqdm(TARGET_ITEMS, desc=\"Processing target items\"):\n    similarities = {}\n    \n    for other_item in all_items:\n        if target_item == other_item:\n            similarities[other_item] = 0.0  # Item with itself\n        else:\n            sim = compute_cosine_similarity_manual(\n                target_item, other_item, \n                item_user_ratings, item_means\n            )\n            similarities[other_item] = sim\n    \n    target_similarities[target_item] = similarities\n\nprint(f\"\\n✓ Similarity calculation completed for {len(TARGET_ITEMS)} target items\")\n\n# Show sample similarities\nfor target_item in TARGET_ITEMS[:3]:\n    sims = target_similarities[target_item]\n    sims_nonzero = {k: v for k, v in sims.items() if v > 0}\n    print(f\"\\nTarget item: {target_item}\")\n    print(f\"  Non-zero similarities: {len(sims_nonzero)}\")\n    if sims_nonzero:\n        top_5 = sorted(sims_nonzero.items(), key=lambda x: x[1], reverse=True)[:5]\n        print(f\"  Top 5 similar items:\")\n        for item, sim in top_5:\n            print(f\"    {item}: {sim:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step 2: Identify Top 20% Similar Items for Each Target Item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For each target item, select top 20% most similar items\ntop_k_percent = int(num_items * 0.2)\n\nprint(f\"Selecting top {top_k_percent} similar items (20% of {num_items:,}) for each target item...\")\n\ntop_k_neighbors_sim = {}\ntop_k_similarities = {}\n\nfor target_item in TARGET_ITEMS:\n    sims = target_similarities[target_item]\n    \n    # Sort by similarity (descending) and take top K\n    sorted_items = sorted(sims.items(), key=lambda x: x[1], reverse=True)\n    \n    # Filter out zero similarities and take top K\n    top_items = [(item, sim) for item, sim in sorted_items if sim > 0][:top_k_percent]\n    \n    neighbor_items = [item for item, sim in top_items]\n    neighbor_sims = [sim for item, sim in top_items]\n    \n    top_k_neighbors_sim[target_item] = neighbor_items\n    top_k_similarities[target_item] = neighbor_sims\n\nprint(f\"\\n✓ Top 20% neighbors identified for all target items\")\n\n# Show statistics\nfor target_item in TARGET_ITEMS:\n    print(f\"\\nTarget item: {target_item}\")\n    print(f\"  Neighbors: {len(top_k_neighbors_sim[target_item])}\")\n    if len(top_k_similarities[target_item]) > 0:\n        print(f\"  Top 5 similarities: {top_k_similarities[target_item][:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize neighborhood sizes\nneighborhood_sizes = [len(top_k_neighbors_sim[item]) for item in TARGET_ITEMS]\n\nplt.figure(figsize=(10, 5))\nplt.bar(range(len(TARGET_ITEMS)), neighborhood_sizes, alpha=0.7)\nplt.xlabel('Target Item Index')\nplt.ylabel('Number of Similar Items')\nplt.title('Neighborhood Sizes for Target Items (Similarity-Based)')\nplt.axhline(np.mean(neighborhood_sizes), color='red', linestyle='--', label=f'Mean: {np.mean(neighborhood_sizes):.0f}')\nplt.xticks(range(len(TARGET_ITEMS)), [f\"{i}\" for i in range(len(TARGET_ITEMS))])\nplt.legend()\nplt.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\nprint(f\"Average neighborhood size: {np.mean(neighborhood_sizes):.0f}\")\nprint(f\"Min: {np.min(neighborhood_sizes)}, Max: {np.max(neighborhood_sizes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step 3: Predict Missing Ratings (Similarity-Based Method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build user rating lookup for predictions\nprint(\"Building user rating lookup...\")\nuser_item_ratings = defaultdict(dict)\nfor _, row in df.iterrows():\n    user_item_ratings[row['user']][row['item']] = row['rating']\n\nprint(f\"User ratings lookup created for {len(user_item_ratings)} users\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_rating(user, target_item, user_item_ratings, neighbors, neighbor_sims, item_means):\n    \"\"\"\n    Predict rating for a user-item pair using item-based CF.\n    \n    Args:\n        user: User ID\n        target_item: Item ID to predict\n        user_item_ratings: Dict {user: {item: rating}}\n        neighbors: List of neighbor item IDs\n        neighbor_sims: List of similarity scores\n        item_means: Dict {item: mean_rating}\n    \n    Returns:\n        Predicted rating\n    \"\"\"\n    if len(neighbors) == 0:\n        return item_means.get(target_item, 3.0)\n    \n    user_ratings = user_item_ratings.get(user, {})\n    \n    # Find which neighbors the user has rated\n    rated_neighbors = []\n    rated_sims = []\n    \n    for i, neighbor_item in enumerate(neighbors):\n        if neighbor_item in user_ratings:\n            rated_neighbors.append(user_ratings[neighbor_item])\n            rated_sims.append(neighbor_sims[i])\n    \n    if len(rated_neighbors) == 0:\n        return item_means.get(target_item, 3.0)\n    \n    rated_neighbors = np.array(rated_neighbors)\n    rated_sims = np.array(rated_sims)\n    \n    # Weighted average\n    if rated_sims.sum() == 0:\n        return item_means.get(target_item, 3.0)\n    \n    prediction = np.sum(rated_sims * rated_neighbors) / np.sum(rated_sims)\n    \n    return np.clip(prediction, 1.0, 5.0)\n\nprint(\"✓ Prediction function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample predictions for target items\nprint(\"Generating sample predictions for target items...\")\n\n# Get ratings for target items\ntarget_item_ratings = df[df['item'].isin(TARGET_ITEMS)]\n\n# Sample some ratings to predict\nsample_size = min(2000, len(target_item_ratings))\nsample_data = target_item_ratings.sample(sample_size, random_state=42)\n\npredictions_sim = []\nactuals = []\n\nfor _, row in tqdm(sample_data.iterrows(), total=len(sample_data), desc=\"Predicting\"):\n    pred = predict_rating(\n        row['user'], row['item'],\n        user_item_ratings,\n        top_k_neighbors_sim[row['item']],\n        top_k_similarities[row['item']],\n        item_means\n    )\n    predictions_sim.append(pred)\n    actuals.append(row['rating'])\n\npredictions_sim = np.array(predictions_sim)\nactuals = np.array(actuals)\n\nprint(f\"\\n✓ Predictions completed ({len(predictions_sim)} predictions)\")\nprint(f\"Prediction range: [{predictions_sim.min():.2f}, {predictions_sim.max():.2f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate evaluation metrics\nmae_sim = np.mean(np.abs(predictions_sim - actuals))\nrmse_sim = np.sqrt(np.mean((predictions_sim - actuals) ** 2))\n\nprint(\"=\"*60)\nprint(\"SIMILARITY-BASED PREDICTION PERFORMANCE\")\nprint(\"=\"*60)\nprint(f\"MAE:  {mae_sim:.4f}\")\nprint(f\"RMSE: {rmse_sim:.4f}\")\nprint(f\"Predictions: {len(predictions_sim)}\")\nprint(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step 4: Compute DF and DS Metrics\n\n**DF (Fixed Constant)**: 53\n\n**DS (Discriminating Score)**: For each item pair:\n- Count number of co-rated users (users who rated both items)\n- DS = min(num_corated_users, DF) / DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define DF constant\nDF = 53\n\nprint(f\"DF (constant): {DF}\")\nprint(f\"\\nCalculating DS scores for target items vs all items...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate co-rating counts and DS scores for target items\nprint(\"Computing co-rating counts and DS scores...\")\n\ntarget_ds_scores = {}\n\nfor target_item in tqdm(TARGET_ITEMS, desc=\"Calculating DS\"):\n    ds_scores = {}\n    \n    target_users = set(item_user_ratings[target_item].keys())\n    \n    for other_item in all_items:\n        if target_item == other_item:\n            ds_scores[other_item] = 0.0\n        else:\n            other_users = set(item_user_ratings[other_item].keys())\n            corated_users = len(target_users & other_users)\n            \n            # DS = min(corated_users, DF) / DF\n            ds = min(corated_users, DF) / DF\n            ds_scores[other_item] = ds\n    \n    target_ds_scores[target_item] = ds_scores\n\nprint(f\"\\n✓ DS scores calculated for {len(TARGET_ITEMS)} target items\")\n\n# Show sample DS statistics\nfor target_item in TARGET_ITEMS[:3]:\n    ds_vals = list(target_ds_scores[target_item].values())\n    ds_nonzero = [v for v in ds_vals if v > 0]\n    print(f\"\\nTarget item: {target_item}\")\n    print(f\"  Non-zero DS scores: {len(ds_nonzero)}\")\n    print(f\"  DS range: [{min(ds_nonzero):.4f}, {max(ds_nonzero):.4f}]\" if ds_nonzero else \"  No non-zero DS scores\")\n    print(f\"  Average DS: {np.mean(ds_nonzero):.4f}\" if ds_nonzero else \"  N/A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize DS distribution\nall_ds_values = []\nfor target_item in TARGET_ITEMS:\n    ds_vals = [v for v in target_ds_scores[target_item].values() if v > 0]\n    all_ds_values.extend(ds_vals)\n\nplt.figure(figsize=(10, 5))\nplt.hist(all_ds_values, bins=50, edgecolor='black', alpha=0.7)\nplt.xlabel('DS Score')\nplt.ylabel('Frequency')\nplt.title('Distribution of DS Scores (Non-zero, All Target Items)')\nplt.axvline(np.mean(all_ds_values), color='red', linestyle='--', label=f'Mean: {np.mean(all_ds_values):.3f}')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step 5: Select Top 20% Items Using DS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For each target item, re-rank neighbors using DS-weighted similarity\nprint(\"Selecting top 20% items using DS-weighted similarity...\")\n\ntop_k_neighbors_ds = {}\ntop_k_scores_ds = {}\n\nfor target_item in TARGET_ITEMS:\n    sims = target_similarities[target_item]\n    ds_scores = target_ds_scores[target_item]\n    \n    # Calculate DS-weighted similarity: similarity * DS\n    weighted_sims = {}\n    for item in all_items:\n        if item != target_item:\n            weighted_sims[item] = sims[item] * ds_scores[item]\n    \n    # Sort and take top K\n    sorted_items = sorted(weighted_sims.items(), key=lambda x: x[1], reverse=True)\n    top_items = [(item, score) for item, score in sorted_items if score > 0][:top_k_percent]\n    \n    neighbor_items = [item for item, score in top_items]\n    neighbor_scores = [score for item, score in top_items]\n    \n    top_k_neighbors_ds[target_item] = neighbor_items\n    top_k_scores_ds[target_item] = neighbor_scores\n\nprint(f\"\\n✓ Top 20% DS-weighted neighbors identified\")\n\n# Show statistics\nfor target_item in TARGET_ITEMS:\n    print(f\"\\nTarget item: {target_item}\")\n    print(f\"  DS-weighted neighbors: {len(top_k_neighbors_ds[target_item])}\")\n    if len(top_k_scores_ds[target_item]) > 0:\n        print(f\"  Top 5 DS-weighted scores: {top_k_scores_ds[target_item][:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare neighborhood sizes\nneighborhood_sizes_ds = [len(top_k_neighbors_ds[item]) for item in TARGET_ITEMS]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].bar(range(len(TARGET_ITEMS)), neighborhood_sizes, alpha=0.7, label='Similarity-based')\naxes[0].set_xlabel('Target Item Index')\naxes[0].set_ylabel('Number of Neighbors')\naxes[0].set_title('Similarity-Based Neighborhoods')\naxes[0].axhline(np.mean(neighborhood_sizes), color='red', linestyle='--', label=f'Mean: {np.mean(neighborhood_sizes):.0f}')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3, axis='y')\n\naxes[1].bar(range(len(TARGET_ITEMS)), neighborhood_sizes_ds, alpha=0.7, color='green', label='DS-weighted')\naxes[1].set_xlabel('Target Item Index')\naxes[1].set_ylabel('Number of Neighbors')\naxes[1].set_title('DS-Weighted Neighborhoods')\naxes[1].axhline(np.mean(neighborhood_sizes_ds), color='red', linestyle='--', label=f'Mean: {np.mean(neighborhood_sizes_ds):.0f}')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Similarity-based average: {np.mean(neighborhood_sizes):.0f}\")\nprint(f\"DS-weighted average: {np.mean(neighborhood_sizes_ds):.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step 6: Updated Rating Predictions Using DS-Filtered Neighborhoods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict using DS-weighted neighborhoods\nprint(\"Generating predictions using DS-weighted method...\")\n\npredictions_ds = []\n\nfor _, row in tqdm(sample_data.iterrows(), total=len(sample_data), desc=\"Predicting\"):\n    pred = predict_rating(\n        row['user'], row['item'],\n        user_item_ratings,\n        top_k_neighbors_ds[row['item']],\n        top_k_scores_ds[row['item']],\n        item_means\n    )\n    predictions_ds.append(pred)\n\npredictions_ds = np.array(predictions_ds)\n\nprint(f\"\\n✓ DS-weighted predictions completed ({len(predictions_ds)} predictions)\")\nprint(f\"Prediction range: [{predictions_ds.min():.2f}, {predictions_ds.max():.2f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate evaluation metrics\nmae_ds = np.mean(np.abs(predictions_ds - actuals))\nrmse_ds = np.sqrt(np.mean((predictions_ds - actuals) ** 2))\n\nprint(\"=\"*60)\nprint(\"DS-WEIGHTED PREDICTION PERFORMANCE\")\nprint(\"=\"*60)\nprint(f\"MAE:  {mae_ds:.4f}\")\nprint(f\"RMSE: {rmse_ds:.4f}\")\nprint(f\"Predictions: {len(predictions_ds)}\")\nprint(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step 7: Compare Similarity Lists from Steps 2 and 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate overlap for each target item\nprint(\"Comparing neighborhood lists...\")\n\njaccard_similarities = []\noverlap_percentages = []\n\nfor target_item in TARGET_ITEMS:\n    neighbors_sim = set(top_k_neighbors_sim[target_item])\n    neighbors_ds = set(top_k_neighbors_ds[target_item])\n    \n    # Jaccard similarity\n    intersection = len(neighbors_sim & neighbors_ds)\n    union = len(neighbors_sim | neighbors_ds)\n    \n    if union > 0:\n        jaccard = intersection / union\n        jaccard_similarities.append(jaccard)\n    \n    # Overlap percentage\n    if len(neighbors_sim) > 0:\n        overlap_pct = intersection / len(neighbors_sim) * 100\n        overlap_percentages.append(overlap_pct)\n    \n    print(f\"\\nTarget item: {target_item}\")\n    print(f\"  Similarity neighbors: {len(neighbors_sim)}\")\n    print(f\"  DS neighbors: {len(neighbors_ds)}\")\n    print(f\"  Intersection: {intersection}\")\n    print(f\"  Jaccard: {jaccard:.4f}\" if union > 0 else \"  Jaccard: N/A\")\n    print(f\"  Overlap: {overlap_pct:.2f}%\" if len(neighbors_sim) > 0 else \"  Overlap: N/A\")\n\njaccard_similarities = np.array(jaccard_similarities)\noverlap_percentages = np.array(overlap_percentages)\n\nprint(f\"\\n\" + \"=\"*60)\nprint(f\"Average Jaccard similarity: {np.mean(jaccard_similarities):.4f}\")\nprint(f\"Average overlap percentage: {np.mean(overlap_percentages):.2f}%\")\nprint(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].bar(range(len(jaccard_similarities)), jaccard_similarities, alpha=0.7, color='purple')\naxes[0].set_xlabel('Target Item Index')\naxes[0].set_ylabel('Jaccard Similarity')\naxes[0].set_title('Jaccard Similarity Between Neighborhood Lists')\naxes[0].axhline(np.mean(jaccard_similarities), color='red', linestyle='--', label=f'Mean: {np.mean(jaccard_similarities):.3f}')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3, axis='y')\n\naxes[1].bar(range(len(overlap_percentages)), overlap_percentages, alpha=0.7, color='orange')\naxes[1].set_xlabel('Target Item Index')\naxes[1].set_ylabel('Overlap Percentage (%)')\naxes[1].set_title('Neighborhood Overlap Percentage')\naxes[1].axhline(np.mean(overlap_percentages), color='red', linestyle='--', label=f'Mean: {np.mean(overlap_percentages):.1f}%')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7 Commentary: Similarity List Comparison\n\n**Key Observations:**\n\nDS-based filtering modifies the neighborhood composition by:\n- Prioritizing items with more co-rated users (up to DF=53)\n- Filtering out items with high similarity but few shared raters\n- Balancing pure similarity with empirical user overlap\n\n**Interpretation:**\n- **High Jaccard/Overlap**: DS filtering confirms original similarity rankings\n- **Low Jaccard/Overlap**: DS significantly restructures neighborhoods\n- Items with dense rating patterns see minimal changes\n- Items with sparse patterns may see dramatic restructuring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step 8: Compare Predicted Ratings from Steps 3 and 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive comparison\nprint(\"=\"*60)\nprint(\"PREDICTION METHOD COMPARISON\")\nprint(\"=\"*60)\nprint(f\"\\nMethod 1: Similarity-Based\")\nprint(f\"   MAE:  {mae_sim:.4f}\")\nprint(f\"   RMSE: {rmse_sim:.4f}\")\nprint(f\"\\nMethod 2: DS-Weighted\")\nprint(f\"   MAE:  {mae_ds:.4f}\")\nprint(f\"   RMSE: {rmse_ds:.4f}\")\nprint(f\"\\nImprovement:\")\nprint(f\"   MAE:  {((mae_sim - mae_ds) / mae_sim * 100):+.2f}%\")\nprint(f\"   RMSE: {((rmse_sim - rmse_ds) / rmse_sim * 100):+.2f}%\")\nprint(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize prediction comparison\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Scatter plots\naxes[0, 0].scatter(actuals, predictions_sim, alpha=0.3, s=10)\naxes[0, 0].plot([1, 5], [1, 5], 'r--', lw=2, label='Perfect')\naxes[0, 0].set_xlabel('Actual Rating')\naxes[0, 0].set_ylabel('Predicted Rating')\naxes[0, 0].set_title(f'Similarity-Based (MAE={mae_sim:.3f})')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].scatter(actuals, predictions_ds, alpha=0.3, s=10, color='green')\naxes[0, 1].plot([1, 5], [1, 5], 'r--', lw=2, label='Perfect')\naxes[0, 1].set_xlabel('Actual Rating')\naxes[0, 1].set_ylabel('Predicted Rating')\naxes[0, 1].set_title(f'DS-Weighted (MAE={mae_ds:.3f})')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Error distributions\nerrors_sim = predictions_sim - actuals\nerrors_ds = predictions_ds - actuals\n\naxes[1, 0].hist(errors_sim, bins=50, edgecolor='black', alpha=0.7)\naxes[1, 0].set_xlabel('Prediction Error')\naxes[1, 0].set_ylabel('Frequency')\naxes[1, 0].set_title('Similarity-Based Errors')\naxes[1, 0].axvline(0, color='red', linestyle='--')\naxes[1, 0].axvline(np.mean(errors_sim), color='orange', linestyle='--', label=f'Mean: {np.mean(errors_sim):.3f}')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].hist(errors_ds, bins=50, edgecolor='black', alpha=0.7, color='green')\naxes[1, 1].set_xlabel('Prediction Error')\naxes[1, 1].set_ylabel('Frequency')\naxes[1, 1].set_title('DS-Weighted Errors')\naxes[1, 1].axvline(0, color='red', linestyle='--')\naxes[1, 1].axvline(np.mean(errors_ds), color='orange', linestyle='--', label=f'Mean: {np.mean(errors_ds):.3f}')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8 Discussion: Prediction Quality Comparison\n\n**Key Findings:**\n\n1. **Performance Impact**: DS filtering shows its effect on prediction accuracy\n2. **Error Distribution**: Both methods have errors centered near zero\n3. **Trade-offs**: Similarity-based uses pure content similarity; DS-weighted balances with co-rating evidence\n\n**Why DS Matters:**\n- Filters items with high similarity but few co-rated users\n- Reduces noise from spurious correlations  \n- DF=53 cap prevents popularity bias\n\n**Practical Takeaway:**\n- Positive improvement → co-rating information is valuable\n- Negative improvement → pure similarity is more reliable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Step 9: Final Commentary and Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comprehensive Analysis\n\n#### Implementation Summary\n\nThis analysis implemented item-based collaborative filtering for **specific target items** using:\n\n1. **Manual Cosine Similarity Calculation** with mean-centering\n2. **Targeted Computation**: Similarity calculated only for target items vs all items\n3. **DS-Based Filtering**: DS = min(corated_users, 53) / 53\n\n#### Key Insights\n\n**Manual Similarity Calculation:**\n- Computed using: `dot(centered_i, centered_j) / (||centered_i|| * ||centered_j||)`\n- Only calculated for target items (not all-to-all)\n- Memory efficient for analyzing specific items\n\n**DS Formula: DS = min(corated_users, DF) / DF**\n- Normalizes co-rating counts to [0, 1]\n- DF=53 caps the benefit of very popular items\n- Balances similarity with empirical overlap\n\n**Impact of DS Filtering:**\n- Modifies neighborhoods based on co-rating evidence\n- Items with >53 co-raters are capped\n- Trade-off between precision and coverage\n\n#### Recommendations\n\n**When to use Similarity-Based:**\n- Very sparse datasets\n- Cold-start scenarios\n- Coverage more important than precision\n\n**When to use DS-Weighted:**\n- Sufficient co-ratings available\n- Reliability is critical\n- Need to filter noise\n\n**Optimal Strategy:**\n- Hybrid approach based on item density\n- Adaptive DF tuning\n- Ensemble methods\n\n#### Conclusion\n\nThis targeted approach allows analyzing specific items efficiently without computing full all-to-all similarity matrices. DS-based filtering provides a principled way to incorporate co-rating evidence, with performance depending on dataset characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\nprint(\"\\n\" + \"=\"*70)\nprint(\" ITEM-BASED COLLABORATIVE FILTERING: FINAL SUMMARY\")\nprint(\"=\"*70)\nprint(f\"\\nDataset: Amazon Movies & TV\")\nprint(f\"  Total ratings: {len(df):,}\")\nprint(f\"  Total users: {df['user'].nunique():,}\")\nprint(f\"  Total items: {num_items:,}\")\nprint(f\"\\nTarget Items Analyzed: {len(TARGET_ITEMS)}\")\nfor i, item in enumerate(TARGET_ITEMS, 1):\n    print(f\"  {i}. {item}\")\nprint(f\"\\nNeighborhood Selection:\")\nprint(f\"  Top K (20%): {top_k_percent} items\")\nprint(f\"  DF constant: {DF}\")\nprint(f\"\\nPrediction Performance (on {len(predictions_sim)} samples):\")\nprint(f\"  Similarity-Based:  MAE={mae_sim:.4f}, RMSE={rmse_sim:.4f}\")\nprint(f\"  DS-Weighted:       MAE={mae_ds:.4f}, RMSE={rmse_ds:.4f}\")\nprint(f\"\\nNeighborhood Overlap:\")\nprint(f\"  Average Jaccard: {np.mean(jaccard_similarities):.4f}\")\nprint(f\"  Average Overlap: {np.mean(overlap_percentages):.2f}%\")\nprint(\"\\n\" + \"=\"*70)\nprint(\" ✓ All 9 steps completed successfully!\")\nprint(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}